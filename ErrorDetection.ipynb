{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Nanodegree Capstone Project\n",
    "\n",
    "#### Author: Quentin Thomas\n",
    "#### Project: Error Detection Experiment\n",
    "#### Date: 8/27/17\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below will be the entire process of building a error detection system. Each section will contain a different stage that the data will go through before finally showing a working example of how the system can be used to detect error messages in random web text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Neccessary Libraries\n",
    "\n",
    "The first step is to import all the neccessary libraries that will be needed throughout the entire process of building the machine learning system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/TheQuengineer/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything we've downloaded is pretty standard for Machine Learning projects. The only thing that is different is the `re` library, and the `nltk` library. The `re` library will be used to work with regular expressions much more easily as our data will consists of text instead of numerical data. `nltk` Is also neccessary so that we can leverage the stopwords that will be needed during data preprocessing. The issue is we don't want to count words like or, the, it, and etc... in our algorithm as they don't tell us much so it is better not to utilize them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration\n",
    "\n",
    "We are attempting to solve a natrual language processing problem, so the dataset will be much different than numerical machine learning data, so there will be no need to get an idea of the standard deviation, mean, or other useful numerical related data from this dataset. Instead it makes more sense to Import the data and then explore it as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Message  IsError?\n",
      "0                                  Resource not found.         1\n",
      "1                                     404 Error thrown         1\n",
      "2    oops... There seems to be a problem loading th...         1\n",
      "3    If you are up for some practical activities, h...         0\n",
      "4    Accuracy obtained in the tutorial. But remembe...         0\n",
      "5                               An error has occurred.         1\n",
      "6                                       Page not found         1\n",
      "7              He needs to learn the error of his ways         0\n",
      "8    Websites have long played with fun 404 pages–t...         0\n",
      "9    Although a user may not want a comedic error m...         0\n",
      "10   Error 404: We couldn't find that page, sorry. ...         1\n",
      "11                        503 Response from the server         1\n",
      "12                                     400 bad request         1\n",
      "13                                 Bad Gateway timeout         1\n",
      "14                                 Service Unavailable         1\n",
      "15   He felt so unavailable when he met his new tea...         0\n",
      "16                          500 internal service error         1\n",
      "17                  I'm sorry, the page is unavailable         1\n",
      "18                                       404 Not Found         1\n",
      "19            The President speaks for himself, Chris.         0\n",
      "20   I’ve spoken, I’ve made my own comments as to o...         0\n",
      "21                                Something went wrong         1\n",
      "22                           An error message occurred         1\n",
      "23                                 Net_SendPaket ERROR         1\n",
      "24                              Something bad happened         0\n",
      "25           When computer starts caring about your TV         0\n",
      "26   An error message is information displayed when...         1\n",
      "27                             Windows failed to start         1\n",
      "28                                   Connection Failed         1\n",
      "29   But it doesn’t confine its colorful language t...         0\n",
      "..                                                 ...       ...\n",
      "470      Unable to contact the server, Try again later         1\n",
      "471               Can't make sense of this information         0\n",
      "472                                      CSS is broken         0\n",
      "473  There has never been a better opportunity to g...         0\n",
      "474    We've spoke earlier about restarting the server         0\n",
      "475           We made several changes to the code base         0\n",
      "476       Some of the files you requested are missing.         0\n",
      "477      I saved every penny to be able to afford this         0\n",
      "478  I love your app, but It seems to be a few thin...         0\n",
      "479  The server just died, please contact your netw...         1\n",
      "480            You gotta figure out what works for you         0\n",
      "481  It takes several iterations to get things work...         0\n",
      "482  Unable to login with username and password pro...         1\n",
      "483                                     Page not found         1\n",
      "484  404 error. Doggone it! The page you're looking...         1\n",
      "485                                     Page Not Found         1\n",
      "486  We’re sorry, we seem to have lost this page, b...         1\n",
      "487                                          NOT FOUND         1\n",
      "488  Sorry we couldn't find that page try searching...         1\n",
      "489  This page isn't available the link you followe...         1\n",
      "490  404 Not Found This page is on vacation... And ...         1\n",
      "491               There might be a issue on the server         0\n",
      "492           Can we call someone to help us fix this.         0\n",
      "493  Not so fast, you should try our new online sal...         0\n",
      "494                                      You have mail         0\n",
      "495    There doesn't seem to be any errors on the page         0\n",
      "496                    Exception handling at its best.         0\n",
      "497  An Exception was thrown on the server. Refresh...         1\n",
      "498                       Application pool unavailable         1\n",
      "499  Concurrent request limit exceeded, please cont...         1\n",
      "\n",
      "[500 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('datasets/error_detection_training.tsv', delimiter='\\t', quoting= 3)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training set consists of two columns, one column represents the message and the other represents the `IsError` flag. Some of the messages are closely related to one another. For example **Applicaition pool unavailable** is clearly an error, but the algorithm should not confuse that with **Fill out the applicaiton in order to be a swim teacher at the pool**. Both of the statements contain some of the same keywords but they are in completely different contexts. We would like our algorithm to recognize the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data PreProcessing & Tokenization\n",
    "\n",
    "The most important initial step is to preprocess the data. Because we are working with text we intend to use the bag of words algorithm to combine together like terms. We will also need to stem each word within the message as we don't want to have multiple variations of the same word. We will need to import some helper libraries from `ntlk` in order to do this successfully and then we will define our data pre-processing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(dataset, column):\n",
    "    \"\"\"The preprocess function takes a dataset of text which is a panda series.\n",
    "       All datasets can be found in the `datasets/` directory. This function\n",
    "       will be responsible for cleaning the text. The cleaning process will involve\n",
    "       stemming the text in the data set and making all the terms lowercase as well\n",
    "       as removing all the unneccessary stop words from the messages in the dataset.\n",
    "       \n",
    "       # Parameters\n",
    "       - dataset : the dataset which could be one of the training files after being imported\n",
    "       - column : a column from the training file, or dataset of interest For example 'Message'\n",
    "       \n",
    "    \"\"\"\n",
    "    messages = []\n",
    "    for i in range(0, len(dataset)):\n",
    "        message = re.sub('[^a-zA-Z0-9]', ' ', dataset[column][i])\n",
    "        message = message.lower()\n",
    "        message = message.split()\n",
    "        ps = PorterStemmer()\n",
    "        message = [ps.stem(text) for text in message if not text in set(stopwords.words('english'))]\n",
    "        message = ' '.join(message)\n",
    "        messages.append(message)\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define our `preprocess_text` function which takes a dataset and a column.We wanted to generalize this as it is important to use this function again for different datasets. The function returns a cleaned version of the text. Note that we wanted to keep any digits that are in the text as error messages are sometimes known to have them. We would like the ability to keep those types of values in our messages as they might give us clues to whether or not something is an error. The output of the cleaned messages is shown below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['resourc found', '404 error thrown', 'oop seem problem load page', 'practic activ littl challeng', 'accuraci obtain tutori rememb accuraci enough', 'error occur', 'page found', 'need learn error way', 'websit long play fun 404 page error page get', 'although user may want comed error messag credit card number correct situat might nice funni', 'error 404 find page sorri found', '503 respons server', '400 bad request', 'bad gateway timeout', 'servic unavail', 'felt unavail met new teacher', '500 intern servic error', 'sorri page unavail', '404 found', 'presid speak chri', 'spoken made comment valu well speech gave state depart past week', 'someth went wrong', 'error messag occur', 'net sendpaket error', 'someth bad happen', 'comput start care tv', 'error messag inform display unexpect condit occur', 'window fail start', 'connect fail', 'confin color languag wrongli enter password', 'part pretti straightforward error messag get end doubl take', 'fail startup ssh session', 'file found', 'programm martin rubli pleas window vista begin', 'process thread termin usual said die', 'actual laugh loud socket error success stori one', 'two error messag must includ', 'handler configur', 'request header long', 'deni filter rule', 'mani url segment', 'content length larg', 'file extens deni', 'request contain high bit charact', 'queri string sequenc deni', 'grandmoth die today famili recov', 'dav request sent static file handl', 'verb deni', 'oh space invad destroy page take reveng', 'look page find', 'oop someth went wrong', 'awkward 404 page found internet', 'page look found', 'oop look like got wrong stop', '404 page found', 'ahhhhhhhhhhh page exist', 'seem find page look', 'web page look', 'page look appear move delet exist', 'thing look', 'style 404 page fit rest site design nice reflect lightheart approach make otherwis bore task someth fun endear', 'click broken link pain witti well design error page sweeten pill', 'brilliantli execut nice interact', 'got 404', 'leav excel impress', 'site worth visit brilliant artwork', 'page', 'anyth truli real make think', 'bret victor 404 page inspir cours ren magritt icon paint treacheri imag confront viewer challeng philosoph question', 'audio effect use 404 capac', 'look someth', 'busi base strong design principl expect well realis 404 page disappoint', 'someth wrong', 'page featur cute littl creatur bob interact sea respond mous movement', 'take face valu see signifi insid natur navig non exist page', 'whoop unexpect', 'good exampl clean approach handl error', 'mayb suggest get plaster', 'web page requir', 'bold typographi make page work well', 'grat broke', 'video game develop blizzard take origin approach 404 page fit gener style use broken glass metaphor broken link', 'unabl locat paramet javascript file', 'context', 'uncaught typeerror undefin function', 'joke mani way character type confus often exist develop', 'javascript code techniqu design pattern', 'would import get object', 'error occur script page', 'done error page', 'runtim error occur', 'uncaught syntax error unexpect token', 'unabl connect', 'secur except', 'server error applic', 'site secur certif trust', 'could convert null int none nullabl data type', '401 unauther access deni due invalid credenti', 'intern error occur unabl load resourc', 'invalid name', 'invalid email', 'stringexcetpion', 'fail load imag', 'parseerror syntax error unexpect', '502 bad gateway', 'servic temporarili unavail', 'due mainten unabl process request time', '404 file found', 'look like page miss', 'intern server error', 'error server respond', 'tell us applic', 'better learn error way', 'page work', 'connect privat', '408 request timeout', 'fatal error occur', 'sorri someth went wrong', '404 accept', 'bad request', 'page display', 'embrac humor situat', 'keep lightheart otherwis may seem like tri cover mistak rather own', 'heavi lift user usual best way ensur great experi', 'sorri find account usernam', 'ignit busi creation web', 'fame money whatev want', 'refer remark tillerson made aug 18 inclus toler discuss racial divers american valu', 'trump condemn hate group wake charlottesvil', 'role respons', 'conor pure box fundament compet 12 full round', 'tomato separ work', 'world popul databas grow', 'cool compil c code', 'easi power revers proxi load balanc docker', 'dub 31 nation televis game come season note number 2017 18 schedul infograph', 'go total wrong', 'tonight damn good fight', 'prepar wors case scenario', 'scientist make shock discoveri yoga mat', 'studi publish onlin august 25 2017 environment health perspect', 'harvard chan author includ lidia nguez alarc n paig william jennif ford', 'common type flame retard associ reduc likelihood clinic pregnanc live birth follow ivf coupl undergo ivf may want opt product flame retard free', 'im sorri done', '90 flower print dress witchi boot est seiz vintag bass', 'shift dynam play big part haim sound meant short session four hour later group still perfect', 'nearli four year worldwid tour realli felt fire band daniel said', 'get children readi hurrican', '5 thing hurrican', 'two round mcgregor look complet home ring', 'mayweath start open third start land fourth start land bunch fifth', 'spur win', 'arash markazi espn com kind enough post compubox offici statist twitter check', 'separ place nearbi', 'brought rippl emot everi take spotifi vocal went control booth instruct engin', 'mainten perform', 'gener error', 'error establish databas connect', 'fatal error occur', 'sorri someth went wrong notifi issu take action shortli', '403 forbidden', 'amp valid error', '404 error look like one els', '404 that error', 'proxi error', 'webpag avail', 'pleas tri 30 second', 'could connect mysql', 'sorri us', '502 web server receiv invalid respons act gateway proxi server', 'sorri find', 'error 522 connect time', 'establish good connect', 'everyth look work order', 'connect success', 'pain much work', 'seem work way want', 'page buggi', '504 gateway time', 'oop click dead link', 'secur connect fail', 'pleas see admin help someth went wrong', 'request document found server', 'ip address request server could found', 'applic fail initi properli', 'page cannot found', 'unknown error occur', 'startup disk almost full', 'unexpect error', 'fail connect server', 'system could find path specifi', 'system moment', 'permiss access', 'request url could receiv', 'page cannot display', 'permiss access server', 'oh page', 'unabl connect', 'connectionist', 'clear error code man', 'common thing care', 'fail make connect one anoth', 'convers spoke expect outcom', 'keep fail make impress', 'http error process failur', 'connect incomplet us', 'man win fight', 'alway big problem tri get stuff work', 'let know want us start', 'yet', 'need tremend amount help', 'human', 'find error messag', 'http statu 403 access deni', 'allow access', 'invalid argument', 'page display', 'mastermind must test', 'still recov broken heart', 'govern websit broken', 'comment put zuckerberg middl contenti polit battl could soon head court', 'earlier post tout zuckerberg immigr root read', 'need keep countri safe focus peopl actual pose threat', 'last month group attorney gener 10 state sent letter trump', 'hawk becam unlik sidekick texa cab driver seek refug insid man vehicl appar refus leav', 'got new puppi', 'find puppi', 'bruso said tri shoo hawk away avail', 'twrc confirm bird rescu said would follow addit inform later sunday', 'spotifi get music', 'self assur vibe come music like readi bounci synthi come onetim lover', 'go est said apologet', 'get sent ton number stat compar econom model', 'say take tour britain let rot', 'alexandru duru softwar engin built record break hoverboard scratch duru develop commerci version omni hoverboard achiev hoverboard dream', 'intuit understand world work intuit work well everyday live quit match realiti', 'idea known macror macroscop object hous still even look', 'open box see head ten second later open box must also see head', 'macror true middl observ alway first last observ alway opposit alway random possibl experi', 'realiti condit express simpl inequ known leggett garg inequ', 'interest quantum system violat inequ time', 'one exampl intuit fail come quantum theori', '5 user tip creat best error messag', 'seem problem', 'ouch hurt 500', '500 intern server error', 'someth went wrong reload page tri', 'site capac sorri inconveni', 'problem resourc look locat', 'oop must miss one 500 intern server error', 'failur occur websit pleas contact admin immedi', 'page work pleas close browser tri', 'server listen', 'team highli train monkey dispatch deal situat', 'error 504 server unavail', 'servlet execut threw except', 'happen upon bit problem 500 intern server error', '500 sorri us', '500 error sorri afraid', 'made 500 buck day', 'pleas fix server error peopl', 'pleas hold tight sorri high traffic caus delay', 'traffic tonight area go significantli busi', 'unabl creat request time pleas check back later', 'bad request receiv', '400 bad request could render', 'request transfer deni', 'gateway timeout occur sorri troubl caus', 'love remot work give us call', 'error encount authent server', 'oauthexcept alias request exist', '400 bad request', 'server encount error mi configur server', 'specifi account alreadi exist', 'specifi account process creat', 'specifi account disabl', 'server fail authent request make sure valu author header form correctli includ signatur', 'condit header support', 'bad request key one metadata key valu pair empti', 'write oper allow', 'specifi resourc name contain invalid charact', 'content length header specifi', 'one request input rang', 'queri paramet specifi request outsid permiss rang', '503 servic unavail', 'previou post seri pragmat rest api design', 'begin determin issu', 'coupl best practic includ exercis', 'experienc error much think', 'provid error code addit valu payload', 'exactli sure mani time fail', 'sorri understand', 'long way go', 'tell time yet watch broken', 'websit broken sorri troubl caus', 'check error messag code', 'keep make bad request', 'error messag complet origin', 'wow success', 'use plain languag descript', 'winter come', 'oper complet', 'sorri android applic process stop unexpectedli', 'error peer certif', 'window limit reach space', 'oh snap make chang submit', 'error invalid server respons code applic close', 'error invalid number enter', 'sorri unabl connect pleas check internet connect', 'login error pleas tri', 'registr error devic confirm respons fail ssl problem encount', 'time redirect attent mistak make', 'think complianc', 'error invalid data', 'remot connect timeout click ok continu', 'unsupport browser firefox chrome internet explor', 'oop mistak', 'tri load truck enough space', 'check network connect proxi phone set tri sign', 'download fail sorri', 'pleas tri might success', 'mobil servic run', 'iphon die', 'troubl log app', 'add new sub account first', 'instal error encount home page', 'someth technic wrong work thing back normal realli soon', 'unabl obtain gp coordin due permiss deni error', 'must make sure secur exactli way need', 'new applic game changer excit', 'access want', 'view pdf tri click download pdf button', 'welcom moe bbq best texa', 'get insight show', 'steel game throne season final', 'dog love tv movi', 'respons fix bug page', 'host vma year', 'receiv unauthor request pleas tri anoth user name password', '402 error payment requir', '408 sorri seem request timeout server', '415 thrown encount unknown media type', '403 forbidden request valid server refus action', '400 000 peopl show dinner', '400 friend show graduat', 'server cannot process request due client error see error code 400', '526 error found seem site invalid ssl certif', 'need renew certif order get job', 'im unabl understand error keep see', 'fatal crash encount server longer respond', '505 peopl show backyard watch eclips', '524 timeout occur server', '523 origin unreach', '522 connect time pleas check back later', '520 unknown error ben trigger contact system administr', 'bandwidth exceed limit 509 thrown', 'better fix load page', 'usual impli futur avail', 'seem conflict schedul', 'reserv futur use', 'founder ceo elon musk announc twitter saturday spacex releas photo futurist space suit', 'much buzz hear spacex recent unman spaceflight', 'inde compani busi work perfect reusabl rocket technolog may forgotten also prepar eventu send human space', 'could ocean land mobil safeti test mean suit design explor beyond insid spacecraft', 'bring back reinvent electr vehicl', 'space found', 'follow recent chang leadership morgan stanley analyst adam jona hope ford soon fulli commit creation electr vehicl', 'ford plan electr vehicl could take turn better new leadership specul morgan stanley analyst adam jona', 'despit optim ford previou announc jona confid compani investor support initi', 'seem fear justifi', 'could properli load css file', 'shortli thereaft april environment protect agenc epa delet mention climat chang websit', 'data clear must act', 'need new fallback imag', 'reflect approach new leadership', 'make someth real talk', 'mani error think', 'thought would better opportun', 'load new tab tri', 'usda reveal leadership instruct staff use phrase', 'result realli good', 'request success', 'seem increas frequenc coastal flood', 'websit heavi load respons present server', 'must chang disk', 'new error messag improv code qualiti', 'use code left behind server', 'mani error', 'american test festiv underway', 'first american built steam locomot', 'beauti error paint', 'show reduc issu server', 'chang fix error project', 'attent need problem process request', 'featur click', 'unauthor request made server contact administr help', 'section 2 describ algorithm properti updat rule', 'data scienc futur', 'special discount amazon prime product', 'never better time build error server', 'order good user experi need make sure leverag success', 'success messag sent', 'messag sent success', 'blame internet buggi', 'unabl load video resourc', 'plugin fail load', 'podcast current load', 'emerg shutdown need', 'server shut', 'login server updat flash plugin', 'wow shame fix site pleas', 'score 32 45 jaguar win', 'bitcoin process rise', 'know ever thought game throne dumb season often', 'end way', 'resourc gener unrecover error', 'search bumpi ride ride hail firm', 'googl self drive vehicl arm waymo mount driver dissatisfact', 'adult enough face cultur issu uber stodgi legaci technolog compani', 'lead appoint uber investor embroil disagr kind ceo uber need', 'insan pluck water resid recal ordeal', 'stori becom disast', 'citi illus circus floyd mayweath jr conor mcgregor pull box spectacl la vega fit time', 'cashless toll could pave way congest price', 'take lifetim master skill sometim', 'sorri video fail load', 'retri refresh screen', 'old news everybodi know site broken time', 'take villag fix issu mani', 'multipl state move sign climat chang debat', 'click ad claim prize', 'icon episod fan', 'select cap contenti search process ride hail compani seek move past turbul period', 'uber compani board expedia respond request comment', 'femal leader also offer opportun uber improv workplac imag', 'maker nvidia artifici intellig softwar maker indoor farm start', 'resourc mr son abl deploy money one deal ventur capit firm dream spend year', 'though privat held compani disclos financi report', 'qi 1 2 power profil allow 15 watt power transfer charger phone', 'said still faster appl 5 watt charger', 'mostli moot switch glass back open possibl ad featur long last might great might expect', 'illeg instruct attempt', 'oper allow cpu', 'fatal error', 'invalid data code access', 'fatal error encount', '404 look like noth found locat', 'system error pleas restart machin', 'unexpect catastroph failur encount pleas kill process', 'catastroph storm swept across texa', 'hard disk error pleas run system diagnost', 'media player cannot play', 'play video get start', 'request abort', 'mr son ad thrill support wework expand across market geographi', 'unabl load ad', 'uber see uptick book', 'make express best', 'go take long time load', 'seem request take longer usual', 'perspect', 'peopl talk look like movi', 'minut later happen', 'due structur damag server', '500 error present endpoint', 'unabl load endpoint', 'resourc unknown server', 'error messag clearli visibl', 'one import thing consid make plan move one citi said jeb kolko chief economist inde', 'could contact server', 'unabl contact server tri later', 'make sens inform', 'css broken', 'never better opportun get tech', 'spoke earlier restart server', 'made sever chang code base', 'file request miss', 'save everi penni abl afford', 'love app seem thing wrong', 'server die pleas contact network administr', 'gotta figur work', 'take sever iter get thing work correctli', 'unabl login usernam password provid', 'page found', '404 error doggon page look cannot found', 'page found', 'sorri seem lost page want lose', 'found', 'sorri find page tri search go home page', 'page avail link follow may broken page remov', '404 found page vacat', 'might issu server', 'call someon help us fix', 'fast tri new onlin sale boutiqu', 'mail', 'seem error page', 'except handl best', 'except thrown server refresh browser tri', 'applic pool unavail', 'concurr request limit exceed pleas contact network administr']\n"
     ]
    }
   ],
   "source": [
    "messages = preprocess_text(dataset, 'Message')\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the messages look much different than before because we applied the stemmer. We also kept our error codes in the text as they can serve as context clues for the algorithm to determine context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization (Bag Of Words)\n",
    "\n",
    "Next we will implement the bag of words so that we can group together like terms for the algorithm. We also want to reduce the number of features that we want the algorithm to use as the more features we have the slower the performance. We want to avoid the curse of dimensionality. As a result we will write a new bag of words and pass our messages as an argument. First we need to import the neccessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bag_of_words(messages, max_features_count):\n",
    "    \"\"\"Takes a list of cleaned text and max features count in order to reduce dimensionality of the matrix\n",
    "       It returns the vectorized data set with the given amount of features\n",
    "       # Parameters\n",
    "       - messages [list]: a list of all the cleaned text\n",
    "       - max_features_count: a tweakable amount of max features that can be tweaked for training purposes.\n",
    "    \"\"\"\n",
    "    #max_features = max_features_count\n",
    "    cv = CountVectorizer(max_features= max_features_count)\n",
    "    X = cv.fit_transform(messages).toarray()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a new function called `bag_of_words` which is responsible for taking the messages and reducing the dimensionality of the data. It will then return the vectorized version of the dataset that can be fed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 500)\n"
     ]
    }
   ],
   "source": [
    "X = bag_of_words(messages, len(messages))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we reduced our dataset down to a sparse `500x500` It could be possible that we would need to lower the dimensionality further, but we will start here with the training of the algorithm. First, we will need the Expected values from the dataset so that it can be fed into the Naive Bayes Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 0 1 1 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 0 1 0\n",
      " 0 1 0 0 1 0 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 0 1 1 1 1 1 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n",
      " 1 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0\n",
      " 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "y = dataset.iloc[:, 1].values\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "Now that we have successfully preprocessed the data we will then look to train our first algorithm. We would like to utilize the Naive Bayes as our benchmark and see if we can obtain a 60% accuracy as our target point. Before we do that we will split the training data up and hold out some for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 111)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our training data split we are ready to begin training our classifier. As mentioned in the proposal our first attempt will be the `Naive Bayes` Algorithm as it is usually the standard algorithm for text classification. Our goal is to get at least a 60% Accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained our model we will evaluate the performance of the model on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating The Model\n",
    "\n",
    "As mentioned in the proposal we will be leveraging a confusion matrix to get the accuracy of the Naive Bayes classifier we have trained. This will give us an idea of whether or not we have reached our accuracy of 60% we've been hunting for. The next section will show our metrics for the Naive Bayes algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[39 15]\n",
      " [ 7 39]]\n"
     ]
    }
   ],
   "source": [
    "nb_y_preds = nb.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "nb_cm = confusion_matrix(y_test, nb_y_preds)\n",
    "\n",
    "print(nb_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our confusion matrix for the Naive Bayes Model shows that we got 22 incorrect predictions out of 100. This gives us an accuracy of roughly 78% on the test set. In order to get a closer look at the accuracy we have obtained we utilized the accuracy score from sklearn to verify further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Naive Bayes Algorithm had an accuracy of 0.78%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "nb_score = accuracy_score(y_test, nb_y_preds)\n",
    "print(\"The Naive Bayes Algorithm had an accuracy of {0}%\".format(nb_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving The Model\n",
    "\n",
    "We have passed our threshold of 60% by a large margin, so we would very much like to save this model so that we might be able to compare it to another. The next cell shows our general function for saving our models during this process, as it is possible we will try another classifier like the random forest classifier to see if it can beat the naive bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "def save_classifier(model, name):\n",
    "    \"\"\"A generic function to make it easier to save models\"\"\"\n",
    "    joblib.dump(model, 'best_models/' + name + '.pkl')\n",
    "    print(\"Model Saved Successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved Successfully.\n"
     ]
    }
   ],
   "source": [
    "save_classifier(nb, \"ErrorDetectionNB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying another Model\n",
    "\n",
    "It is possible that there is another model that will perform better than the `Naive Bayes Classifier` we've chose to get the accuracy greater than 60%. As mentioned in our proposal we wanted to see if the performance could be pushed to an even better place, so we set out to test what would happen if we did a grid search on our random forest hyper parameters and then we fed them to the algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rf_class = RandomForestClassifier(random_state=111)\n",
    "params = {\n",
    "    'n_estimators': [10, 20, 30],\n",
    "    'max_features': [2, 100, 300, 500],\n",
    "    'max_depth': [5, 10, 15]\n",
    "}\n",
    "\n",
    "rf_cv = GridSearchCV(estimator=rf_class, param_grid=params, cv=10, n_jobs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've set up grid search with random forest, we will once again use the dataset to train our new model on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf = rf_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've trained our new model we will take a quick look at the best parameters it found during training.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 15, 'max_features': 500, 'n_estimators': 20}\n",
      "0.76\n"
     ]
    }
   ],
   "source": [
    "print(rf.best_params_)\n",
    "print(rf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that the best parameters to use are a max depth of 15, while using 300 maximum features. The number of estimators it said to use was 20. This approach gave it an accuray of 75% during training which is still better than the 60% we set out to acheive. Now its time to make predictions with the new model and make a final evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Model Evaluation\n",
    "\n",
    "The new model will now need to be tested to verify how it stands up to the Naive Bayes Classifier Utilizing the feedback we got from our grid search algorithm. Below is what we've found after evaluating the new model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[49  5]\n",
      " [ 8 38]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "new_rf = RandomForestClassifier(random_state = 111, max_depth= 15, max_features=500, n_estimators=20)\n",
    "new_rf.fit(X_train, y_train)\n",
    "rf_y_preds = new_rf.predict(X_test)\n",
    "\n",
    "rf_cm = confusion_matrix(y_test, rf_y_preds)\n",
    "\n",
    "print(rf_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix above shows that the Random Forest classifier had a better performance on the test set than the Naive Bayes classifier had. The actual accuracy was obtained by running the code below...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Random Forest classifier had an accuracy score of 0.87%\n",
      "Model Saved Successfully.\n"
     ]
    }
   ],
   "source": [
    "rf_score = accuracy_score(y_test, rf_y_preds)\n",
    "print(\"The Random Forest classifier had an accuracy score of {0}%\".format(rf_score))\n",
    "save_classifier(new_rf, 'ErrorDetectionRF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a working model that is best fit for an error detector classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "We can conclude that it is possible to build a error detection system that has the ability to decipher error messages form the web or any other place it may encounter error text. We have shown that the best algorthm for this problem is The `RandomForestClassifier` as it showed a higher accuracy score on the test set than the Naive Bayes classifier. Some things to try in the future are experiementing with more data, and perhaps error messages in other languages as this model was only designed to work for english based error messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
